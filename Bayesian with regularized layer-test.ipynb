{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85db2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cb4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.2.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\yaswa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.1)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yaswa\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0c2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassificationModel:\n",
    "    def __init__(self, data_file):\n",
    "        # Load the dataset\n",
    "        self.df = pd.read_csv(data_file)\n",
    "        self.df = self.df.drop_duplicates('Tweet', keep='first')\n",
    "        self.df[\"Cleaned_Tweet\"] = [re.sub(r\"(@[A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+\", \"\", v.lower()) for v in self.df[\"Tweet\"]]\n",
    "        self.df.drop('Tweet', axis=1, inplace=True)\n",
    "        vals_to_replace = {'Relevant': 1, 'Not Relevant': 0}\n",
    "        self.df['Category'] = self.df['Category'].map(vals_to_replace)\n",
    "        self.df.dropna(inplace=True)\n",
    "        self.df['Category'].unique()\n",
    "\n",
    "        # Train-test split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.df['Cleaned_Tweet'], self.df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "        # Initialize Tokenizer\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.X_train)\n",
    "        self.max_sequence_length = 100\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Tokenize and pad the sequences\n",
    "        X_train_sequences = self.tokenizer.texts_to_sequences(self.X_train)\n",
    "        X_test_sequences = self.tokenizer.texts_to_sequences(self.X_test)\n",
    "        self.X_train_padded = pad_sequences(X_train_sequences, maxlen=self.max_sequence_length)\n",
    "        self.X_test_padded = pad_sequences(X_test_sequences, maxlen=self.max_sequence_length)\n",
    "\n",
    "    def build_model(self, optimizer, dropout_rate, lstm_units):\n",
    "        # Build the LSTM model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(len(self.tokenizer.word_index) + 1, 32, input_length=self.max_sequence_length))\n",
    "        self.model.add(LSTM(lstm_units, return_sequences=True))\n",
    "        self.model.add(LSTM(lstm_units))\n",
    "        self.model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization\n",
    "        self.model.add(Dropout(dropout_rate))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    def train_model(self, epochs=5, batch_size=32):\n",
    "        # Train the LSTM model\n",
    "        history = self.model.fit(self.X_train_padded, self.y_train, validation_data=(self.X_test_padded, self.y_test), epochs=epochs, batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = self.model.predict(self.X_test_padded)\n",
    "        auc = roc_auc_score(self.y_test, y_pred)\n",
    "        print(\"AUC:\", auc)\n",
    "        return y_pred\n",
    "\n",
    "    def plot_roc_curve(self, y_pred):\n",
    "        # Plot the ROC curve\n",
    "        auc = roc_auc_score(self.y_test, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, y_pred)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learning_curves(self, history):\n",
    "        # Plot the learning curves\n",
    "        plt.figure()\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def bayesian_optimization(self):\n",
    "        # Define the hyperparameter search space\n",
    "        pbounds = {\n",
    "            'learning_rate': (1e-5, 1e-2),\n",
    "            'dropout_rate': (0.1, 0.5),\n",
    "            'lstm_units': (32, 128)\n",
    "        }\n",
    "\n",
    "        def evaluate_model(learning_rate, dropout_rate, lstm_units):\n",
    "            # Build the model with the given hyperparameters\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "            self.build_model(optimizer, dropout_rate, int(lstm_units))\n",
    "            # Train the model with the current hyperparameters\n",
    "            history = self.train_model(epochs=5, batch_size=32)\n",
    "            # Return the validation accuracy as the objective to maximize\n",
    "            return history.history['val_accuracy'][-1]\n",
    "\n",
    "        # Initialize Bayesian Optimization\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=evaluate_model,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2,\n",
    "            random_state=42\n",
    "        )\n",
    "        # Perform optimization\n",
    "        optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_hyperparams = optimizer.max['params']\n",
    "        print(\"Best Hyperparameters:\")\n",
    "        print(best_hyperparams)\n",
    "\n",
    "        # Train the model with the best hyperparameters and more epochs\n",
    "        optimizer = Adam(learning_rate=best_hyperparams['learning_rate'])\n",
    "        self.build_model(optimizer, best_hyperparams['dropout_rate'], int(best_hyperparams['lstm_units']))\n",
    "        history = self.train_model(epochs=100, batch_size=32)\n",
    "\n",
    "        # Evaluate and plot ROC curve with the best model\n",
    "        y_pred = self.evaluate_model()\n",
    "        self.plot_roc_curve(y_pred)\n",
    "\n",
    "        # Plot learning curves of the best model\n",
    "        self.plot_learning_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... | learni... | lstm_u... |\n",
      "-------------------------------------------------------------\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 96s 355ms/step - loss: 0.6601 - accuracy: 0.6498 - val_loss: 0.6720 - val_accuracy: 0.5812\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 87s 343ms/step - loss: 0.4894 - accuracy: 0.8013 - val_loss: 0.5379 - val_accuracy: 0.7881\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 90s 357ms/step - loss: 0.3312 - accuracy: 0.8939 - val_loss: 0.5353 - val_accuracy: 0.7798\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 92s 362ms/step - loss: 0.2276 - accuracy: 0.9373 - val_loss: 0.6207 - val_accuracy: 0.7640\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 95s 374ms/step - loss: 0.1654 - accuracy: 0.9532 - val_loss: 0.6997 - val_accuracy: 0.7807\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7807   \u001b[0m | \u001b[0m0.2498   \u001b[0m | \u001b[0m0.009508 \u001b[0m | \u001b[0m102.3    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 43s 144ms/step - loss: 0.6352 - accuracy: 0.7363 - val_loss: 0.4493 - val_accuracy: 0.8128\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 32s 125ms/step - loss: 0.3315 - accuracy: 0.8729 - val_loss: 0.4621 - val_accuracy: 0.8089\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 25s 97ms/step - loss: 0.2161 - accuracy: 0.9295 - val_loss: 0.6048 - val_accuracy: 0.7926\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 26s 103ms/step - loss: 0.1578 - accuracy: 0.9512 - val_loss: 0.5346 - val_accuracy: 0.7802\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 27s 105ms/step - loss: 0.1180 - accuracy: 0.9662 - val_loss: 0.6406 - val_accuracy: 0.7773\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7773   \u001b[0m | \u001b[0m0.3395   \u001b[0m | \u001b[0m0.001569 \u001b[0m | \u001b[0m46.98    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 98s 361ms/step - loss: 0.5742 - accuracy: 0.7471 - val_loss: 0.5232 - val_accuracy: 0.7649\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 83s 326ms/step - loss: 0.3326 - accuracy: 0.8791 - val_loss: 0.6112 - val_accuracy: 0.7852\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 82s 324ms/step - loss: 0.2164 - accuracy: 0.9309 - val_loss: 0.6276 - val_accuracy: 0.7921\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 96s 379ms/step - loss: 0.1432 - accuracy: 0.9559 - val_loss: 0.7020 - val_accuracy: 0.7827\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 93s 365ms/step - loss: 0.1019 - accuracy: 0.9670 - val_loss: 0.8261 - val_accuracy: 0.7778\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7778   \u001b[0m | \u001b[0m0.1232   \u001b[0m | \u001b[0m0.008663 \u001b[0m | \u001b[0m89.71    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 109s 407ms/step - loss: 1.2037 - accuracy: 0.6219 - val_loss: 0.8447 - val_accuracy: 0.7654\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 97s 381ms/step - loss: 0.6546 - accuracy: 0.8189 - val_loss: 0.5951 - val_accuracy: 0.7921\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 95s 375ms/step - loss: 0.4195 - accuracy: 0.8753 - val_loss: 0.5387 - val_accuracy: 0.7941\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 46s 182ms/step - loss: 0.2903 - accuracy: 0.9137 - val_loss: 0.5389 - val_accuracy: 0.7921\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.2105 - accuracy: 0.9384 - val_loss: 0.5752 - val_accuracy: 0.7822\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.7822   \u001b[0m | \u001b[95m0.3832   \u001b[0m | \u001b[95m0.0002156\u001b[0m | \u001b[95m125.1    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 30s 98ms/step - loss: 0.6052 - accuracy: 0.7448 - val_loss: 0.4541 - val_accuracy: 0.8133\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 21s 84ms/step - loss: 0.3283 - accuracy: 0.8802 - val_loss: 0.4714 - val_accuracy: 0.7980\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 20s 78ms/step - loss: 0.2197 - accuracy: 0.9302 - val_loss: 0.5045 - val_accuracy: 0.7896\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 23s 89ms/step - loss: 0.1584 - accuracy: 0.9537 - val_loss: 0.6795 - val_accuracy: 0.7778\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 22s 88ms/step - loss: 0.1071 - accuracy: 0.9693 - val_loss: 0.7746 - val_accuracy: 0.7714\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.7714   \u001b[0m | \u001b[0m0.433    \u001b[0m | \u001b[0m0.002131 \u001b[0m | \u001b[0m49.46    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 79s 291ms/step - loss: 0.6061 - accuracy: 0.7402 - val_loss: 0.4778 - val_accuracy: 0.7877\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 67s 263ms/step - loss: 0.3434 - accuracy: 0.8748 - val_loss: 0.4897 - val_accuracy: 0.8020\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 60s 236ms/step - loss: 0.2299 - accuracy: 0.9267 - val_loss: 0.6116 - val_accuracy: 0.7936\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 67s 263ms/step - loss: 0.1611 - accuracy: 0.9495 - val_loss: 0.6106 - val_accuracy: 0.7896\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 63s 249ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.6843 - val_accuracy: 0.7793\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.7793   \u001b[0m | \u001b[0m0.4817   \u001b[0m | \u001b[0m0.002821 \u001b[0m | \u001b[0m126.6    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 74s 276ms/step - loss: 0.6142 - accuracy: 0.7098 - val_loss: 0.5025 - val_accuracy: 0.7773\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 77s 305ms/step - loss: 0.3706 - accuracy: 0.8639 - val_loss: 0.5289 - val_accuracy: 0.7936\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 73s 286ms/step - loss: 0.2370 - accuracy: 0.9242 - val_loss: 0.6026 - val_accuracy: 0.7901\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 69s 273ms/step - loss: 0.1939 - accuracy: 0.9422 - val_loss: 0.6939 - val_accuracy: 0.7847\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 70s 275ms/step - loss: 0.1265 - accuracy: 0.9596 - val_loss: 0.7731 - val_accuracy: 0.7748\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.7748   \u001b[0m | \u001b[0m0.341    \u001b[0m | \u001b[0m0.007096 \u001b[0m | \u001b[0m123.3    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 78s 287ms/step - loss: 0.5834 - accuracy: 0.7418 - val_loss: 0.4582 - val_accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 69s 271ms/step - loss: 0.3302 - accuracy: 0.8758 - val_loss: 0.4913 - val_accuracy: 0.7956\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 69s 274ms/step - loss: 0.2149 - accuracy: 0.9326 - val_loss: 0.5647 - val_accuracy: 0.7753\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 76s 302ms/step - loss: 0.1580 - accuracy: 0.9506 - val_loss: 0.6837 - val_accuracy: 0.7733\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 76s 300ms/step - loss: 0.1092 - accuracy: 0.9684 - val_loss: 0.6663 - val_accuracy: 0.7635\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7635   \u001b[0m | \u001b[0m0.1433   \u001b[0m | \u001b[0m0.00364  \u001b[0m | \u001b[0m125.7    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 41s 142ms/step - loss: 0.5842 - accuracy: 0.7317 - val_loss: 0.4562 - val_accuracy: 0.8025\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 33s 130ms/step - loss: 0.3423 - accuracy: 0.8754 - val_loss: 0.5042 - val_accuracy: 0.7926\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 32s 126ms/step - loss: 0.2159 - accuracy: 0.9332 - val_loss: 0.5678 - val_accuracy: 0.7886\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 31s 123ms/step - loss: 0.1469 - accuracy: 0.9581 - val_loss: 0.5921 - val_accuracy: 0.7842\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 30s 117ms/step - loss: 0.1036 - accuracy: 0.9662 - val_loss: 0.9143 - val_accuracy: 0.7719\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7719   \u001b[0m | \u001b[0m0.4772   \u001b[0m | \u001b[0m0.003685 \u001b[0m | \u001b[0m53.24    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 62s 225ms/step - loss: 0.5717 - accuracy: 0.7496 - val_loss: 0.4713 - val_accuracy: 0.8049\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 39s 153ms/step - loss: 0.3387 - accuracy: 0.8846 - val_loss: 0.5552 - val_accuracy: 0.8000\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 42s 167ms/step - loss: 0.2308 - accuracy: 0.9272 - val_loss: 0.6186 - val_accuracy: 0.7842\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 50s 196ms/step - loss: 0.1589 - accuracy: 0.9525 - val_loss: 0.8091 - val_accuracy: 0.7956\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 52s 205ms/step - loss: 0.1196 - accuracy: 0.9636 - val_loss: 0.9946 - val_accuracy: 0.7921\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m0.7921   \u001b[0m | \u001b[95m0.4576   \u001b[0m | \u001b[95m0.008616 \u001b[0m | \u001b[95m71.67    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 103s 387ms/step - loss: 0.5758 - accuracy: 0.7534 - val_loss: 0.7206 - val_accuracy: 0.7047\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 103s 404ms/step - loss: 0.6556 - accuracy: 0.6285 - val_loss: 0.6247 - val_accuracy: 0.6998\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 100s 393ms/step - loss: 0.4158 - accuracy: 0.8476 - val_loss: 0.5578 - val_accuracy: 0.7669\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 97s 382ms/step - loss: 0.2996 - accuracy: 0.9010 - val_loss: 0.5899 - val_accuracy: 0.7788\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 104s 411ms/step - loss: 0.2178 - accuracy: 0.9342 - val_loss: 0.6252 - val_accuracy: 0.7778\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7778   \u001b[0m | \u001b[0m0.2329   \u001b[0m | \u001b[0m0.006802 \u001b[0m | \u001b[0m124.4    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 81s 299ms/step - loss: 0.5660 - accuracy: 0.7439 - val_loss: 0.4821 - val_accuracy: 0.7881\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 74s 290ms/step - loss: 0.3233 - accuracy: 0.8802 - val_loss: 0.5284 - val_accuracy: 0.7867\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 70s 276ms/step - loss: 0.1961 - accuracy: 0.9370 - val_loss: 0.6299 - val_accuracy: 0.7847\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 54s 211ms/step - loss: 0.1181 - accuracy: 0.9622 - val_loss: 0.7280 - val_accuracy: 0.7738\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 67s 262ms/step - loss: 0.0988 - accuracy: 0.9681 - val_loss: 0.8779 - val_accuracy: 0.7783\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7783   \u001b[0m | \u001b[0m0.1378   \u001b[0m | \u001b[0m0.008015 \u001b[0m | \u001b[0m89.19    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 89s 332ms/step - loss: 0.5746 - accuracy: 0.7461 - val_loss: 0.4844 - val_accuracy: 0.7837\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 78s 307ms/step - loss: 0.3348 - accuracy: 0.8821 - val_loss: 0.5331 - val_accuracy: 0.7852\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 59s 232ms/step - loss: 0.2188 - accuracy: 0.9272 - val_loss: 0.6894 - val_accuracy: 0.7768\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 60s 237ms/step - loss: 0.1561 - accuracy: 0.9491 - val_loss: 0.8744 - val_accuracy: 0.7862\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 71s 278ms/step - loss: 0.1007 - accuracy: 0.9669 - val_loss: 0.9608 - val_accuracy: 0.7832\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.7832   \u001b[0m | \u001b[0m0.2612   \u001b[0m | \u001b[0m0.008409 \u001b[0m | \u001b[0m104.7    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 92s 344ms/step - loss: 0.6430 - accuracy: 0.7311 - val_loss: 0.4809 - val_accuracy: 0.7970\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 76s 297ms/step - loss: 0.3373 - accuracy: 0.8732 - val_loss: 0.4811 - val_accuracy: 0.7901\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 76s 300ms/step - loss: 0.2263 - accuracy: 0.9262 - val_loss: 0.5252 - val_accuracy: 0.7970\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 70s 276ms/step - loss: 0.1475 - accuracy: 0.9531 - val_loss: 0.7686 - val_accuracy: 0.7758\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 80s 315ms/step - loss: 0.1026 - accuracy: 0.9670 - val_loss: 0.7765 - val_accuracy: 0.7793\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7793   \u001b[0m | \u001b[0m0.3441   \u001b[0m | \u001b[0m0.002222 \u001b[0m | \u001b[0m125.7    \u001b[0m |\n",
      "Epoch 1/5\n",
      "254/254 [==============================] - 72s 264ms/step - loss: 0.5658 - accuracy: 0.7575 - val_loss: 0.5644 - val_accuracy: 0.7773\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 60s 238ms/step - loss: 0.3213 - accuracy: 0.8848 - val_loss: 0.5248 - val_accuracy: 0.7842\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 61s 241ms/step - loss: 0.2064 - accuracy: 0.9300 - val_loss: 0.7517 - val_accuracy: 0.7748\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 68s 267ms/step - loss: 0.1458 - accuracy: 0.9514 - val_loss: 0.8087 - val_accuracy: 0.7684\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 70s 274ms/step - loss: 0.0999 - accuracy: 0.9665 - val_loss: 0.8455 - val_accuracy: 0.7911\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7911   \u001b[0m | \u001b[0m0.1163   \u001b[0m | \u001b[0m0.004894 \u001b[0m | \u001b[0m88.8     \u001b[0m |\n",
      "=============================================================\n",
      "Best Hyperparameters:\n",
      "{'dropout_rate': 0.4576388678472847, 'learning_rate': 0.008616480887496067, 'lstm_units': 71.6697053331085}\n",
      "Epoch 1/100\n",
      "254/254 [==============================] - 54s 193ms/step - loss: 0.5937 - accuracy: 0.7192 - val_loss: 0.5140 - val_accuracy: 0.7916\n",
      "Epoch 2/100\n",
      "254/254 [==============================] - 42s 166ms/step - loss: 0.3598 - accuracy: 0.8675 - val_loss: 0.5269 - val_accuracy: 0.7975\n",
      "Epoch 3/100\n",
      "254/254 [==============================] - 46s 180ms/step - loss: 0.2422 - accuracy: 0.9251 - val_loss: 0.6117 - val_accuracy: 0.7896\n",
      "Epoch 4/100\n",
      "254/254 [==============================] - 47s 186ms/step - loss: 0.1587 - accuracy: 0.9506 - val_loss: 0.6650 - val_accuracy: 0.7891\n",
      "Epoch 5/100\n",
      "254/254 [==============================] - 48s 187ms/step - loss: 0.1131 - accuracy: 0.9678 - val_loss: 0.9203 - val_accuracy: 0.7654\n",
      "Epoch 6/100\n",
      "254/254 [==============================] - 45s 179ms/step - loss: 0.0919 - accuracy: 0.9691 - val_loss: 1.0786 - val_accuracy: 0.7738\n",
      "Epoch 7/100\n",
      "254/254 [==============================] - 43s 169ms/step - loss: 0.0806 - accuracy: 0.9733 - val_loss: 0.9701 - val_accuracy: 0.7709\n",
      "Epoch 8/100\n",
      "254/254 [==============================] - 46s 180ms/step - loss: 0.0639 - accuracy: 0.9780 - val_loss: 1.0577 - val_accuracy: 0.7674\n",
      "Epoch 9/100\n",
      "254/254 [==============================] - 46s 181ms/step - loss: 0.0583 - accuracy: 0.9800 - val_loss: 1.3361 - val_accuracy: 0.7595\n",
      "Epoch 10/100\n",
      "254/254 [==============================] - 42s 164ms/step - loss: 0.0620 - accuracy: 0.9796 - val_loss: 1.9003 - val_accuracy: 0.7694\n",
      "Epoch 11/100\n",
      "254/254 [==============================] - 44s 173ms/step - loss: 0.0608 - accuracy: 0.9804 - val_loss: 1.3187 - val_accuracy: 0.7684\n",
      "Epoch 12/100\n",
      "254/254 [==============================] - 44s 174ms/step - loss: 0.0487 - accuracy: 0.9828 - val_loss: 1.4864 - val_accuracy: 0.7743\n",
      "Epoch 13/100\n",
      "254/254 [==============================] - 42s 167ms/step - loss: 0.0508 - accuracy: 0.9843 - val_loss: 1.4555 - val_accuracy: 0.7719\n",
      "Epoch 14/100\n",
      "254/254 [==============================] - 43s 171ms/step - loss: 0.0498 - accuracy: 0.9844 - val_loss: 1.5874 - val_accuracy: 0.7714\n",
      "Epoch 15/100\n",
      "254/254 [==============================] - 46s 180ms/step - loss: 0.0634 - accuracy: 0.9805 - val_loss: 1.5404 - val_accuracy: 0.7684\n",
      "Epoch 16/100\n",
      "254/254 [==============================] - 44s 173ms/step - loss: 0.0802 - accuracy: 0.9764 - val_loss: 1.6702 - val_accuracy: 0.7723\n",
      "Epoch 17/100\n",
      "254/254 [==============================] - 44s 172ms/step - loss: 0.0702 - accuracy: 0.9786 - val_loss: 1.2483 - val_accuracy: 0.7625\n",
      "Epoch 18/100\n",
      "254/254 [==============================] - 43s 168ms/step - loss: 0.0577 - accuracy: 0.9806 - val_loss: 1.5955 - val_accuracy: 0.7585\n",
      "Epoch 19/100\n",
      "254/254 [==============================] - 44s 172ms/step - loss: 0.0595 - accuracy: 0.9825 - val_loss: 1.6358 - val_accuracy: 0.7640\n",
      "Epoch 20/100\n",
      "254/254 [==============================] - 42s 166ms/step - loss: 0.0546 - accuracy: 0.9831 - val_loss: 1.7568 - val_accuracy: 0.7748\n",
      "Epoch 21/100\n",
      "254/254 [==============================] - 44s 175ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 1.8928 - val_accuracy: 0.7699\n",
      "Epoch 22/100\n",
      "254/254 [==============================] - 45s 179ms/step - loss: 0.0503 - accuracy: 0.9825 - val_loss: 1.6688 - val_accuracy: 0.7709\n",
      "Epoch 23/100\n",
      "254/254 [==============================] - 43s 168ms/step - loss: 0.0424 - accuracy: 0.9843 - val_loss: 2.2531 - val_accuracy: 0.7649\n",
      "Epoch 24/100\n",
      "254/254 [==============================] - 44s 172ms/step - loss: 0.0454 - accuracy: 0.9839 - val_loss: 1.5950 - val_accuracy: 0.7615\n",
      "Epoch 25/100\n",
      "254/254 [==============================] - 45s 177ms/step - loss: 0.0648 - accuracy: 0.9788 - val_loss: 2.0222 - val_accuracy: 0.7694\n",
      "Epoch 26/100\n",
      "254/254 [==============================] - 46s 183ms/step - loss: 0.0518 - accuracy: 0.9830 - val_loss: 2.7952 - val_accuracy: 0.7659\n",
      "Epoch 27/100\n",
      "254/254 [==============================] - 41s 162ms/step - loss: 0.0670 - accuracy: 0.9800 - val_loss: 2.1847 - val_accuracy: 0.7481\n",
      "Epoch 28/100\n",
      "254/254 [==============================] - 42s 165ms/step - loss: 0.0597 - accuracy: 0.9800 - val_loss: 1.8848 - val_accuracy: 0.7600\n",
      "Epoch 29/100\n",
      "254/254 [==============================] - 39s 155ms/step - loss: 0.0591 - accuracy: 0.9795 - val_loss: 2.2299 - val_accuracy: 0.7472\n",
      "Epoch 30/100\n",
      "254/254 [==============================] - 41s 163ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 2.4552 - val_accuracy: 0.7526\n",
      "Epoch 31/100\n",
      "254/254 [==============================] - 43s 169ms/step - loss: 0.0531 - accuracy: 0.9812 - val_loss: 2.0263 - val_accuracy: 0.7299\n",
      "Epoch 32/100\n",
      "254/254 [==============================] - 45s 178ms/step - loss: 0.0732 - accuracy: 0.9785 - val_loss: 2.2350 - val_accuracy: 0.7551\n",
      "Epoch 33/100\n",
      "254/254 [==============================] - 39s 155ms/step - loss: 0.0791 - accuracy: 0.9742 - val_loss: 1.8004 - val_accuracy: 0.7402\n",
      "Epoch 34/100\n",
      "254/254 [==============================] - 40s 156ms/step - loss: 0.0717 - accuracy: 0.9789 - val_loss: 2.3438 - val_accuracy: 0.7457\n",
      "Epoch 35/100\n",
      "254/254 [==============================] - 44s 173ms/step - loss: 0.0695 - accuracy: 0.9780 - val_loss: 2.1560 - val_accuracy: 0.7526\n",
      "Epoch 36/100\n",
      "250/254 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9769"
     ]
    }
   ],
   "source": [
    "data_file = \"C:/Users/yaswa/Downloads/Group3-news-text-sanitized.csv\"\n",
    "model = TweetClassificationModel(data_file)\n",
    "model.preprocess_data()\n",
    "model.bayesian_optimization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807e10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
